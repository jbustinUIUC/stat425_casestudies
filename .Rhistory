rmse(predict(phys.pcr, ncomp=6), model_train$log_physicians)
rmse(predict(phys.pcr, model_test, ncomp=6), model_test$num_physicians)
rmse(predict(phys.pcr, model_test, ncomp=6), model_test$log_physicians)
train.colnames
str(train)
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_physicians", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
# Create Table for Results
results <- data.frame(matrix(ncol = , nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_physicians", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
# Create Table for Results
results <- data.frame(matrix(ncol = 13, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_physicians", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:13] <- TRUE
View(results)
colnames(rs$which)[rs$which[8,]]
str(region)
str(train)
# Reading Transformed Data In From CS1
sub1_data<-read.csv("sub1_data")
# Convert Region to factor level
sub1_data <- sub1_data %>% mutate(region = as.factor(region))
# Setting Seed
set.seed(425)
# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(sub1_data), replace=TRUE, prob=c(0.7, 0.3))
train <- sub1_data[sample, ]
test <- sub1_data[!sample, ]
# Create Table for Results
results <- data.frame(matrix(ncol = 13, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_physicians", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
#Creating model using training data
cs1_model<-lm(log_physicians ~ ., data=train)
#Creating function to calculate RMSE
rmse<-function(x,y) sqrt(mean((x-y)^2))
#Train MSE
rmse(fitted(cs1_model), train$log_physicians)
#Test MSE
rmse(predict(cs1_model, test), test$log_physicians)
results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:13] <- TRUE
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)
# Adjusted-R2, 8th is best
rs$adjr2
# BIC, 4th is best
# Note that Leaps does not calculate AIC.
rs$bic
#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8
AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best
colnames(rs$which)[rs$which[8,]]
View(results)
str(sub1_data)
# Reading Transformed Data In From CS1
sub1_data<-read.csv("sub1_data")
# Convert Region to factor level
sub1_data <- sub1_data %>% mutate(region = as.factor(region))
# Setting Seed
set.seed(425)
# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(sub1_data), replace=TRUE, prob=c(0.7, 0.3))
train <- sub1_data[sample, ]
test <- sub1_data[!sample, ]
# Create Table for Results
results <- data.frame(matrix(ncol = 12, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
#Creating model using training data
cs1_model<-lm(log_physicians ~ ., data=train)
#Creating function to calculate RMSE
rmse<-function(x,y) sqrt(mean((x-y)^2))
#Train MSE
rmse(fitted(cs1_model), train$log_physicians)
#Test MSE
rmse(predict(cs1_model, test), test$log_physicians)
results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:13] <- TRUE
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)
# Adjusted-R2, 8th is best
rs$adjr2
# BIC, 2nd is best
# Note that this is not the same as our calculated BIC
rs$bic
#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8
AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best
par(mfrow=c(2,2))
plot(msize, rs$adjr2, xlab="No. of Parameters", ylab = "Adjusted Rsquare");
plot(msize, rs$cp, xlab="No. of Parameters", ylab = "Mallow's Cp");
plot(msize, AIC, xlab="No. of Parameters", ylab = "AIC");
plot(msize, BIC, xlab="No. of Parameters", ylab = "BIC");
#Determining which variables to keep based
#Because both Adjusted R2 and AIC suggested 8 variables, we will choose 8 variables
rs$which[8,]
select.var = colnames(rs$which)[rs$which[8,]]
select.var = select.var[-1]
#fitting model
criteria_fit <- lm(log_physicians ~ . , data=train[, c(select.var, "log_physicians")])
select.var = colnames(rs$which)[rs$which[8,]]
select.var = select.var[-1]
select_vars
select.vars
select.var
results[2,1] <- "Citerion Selected Model"
results[2,2] <- rmse(fitted(criteria_fit), train$log_physicians)
results[2,3] <- rmse(predict(criteria_fit, test), test$log_physicians)
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)
# Adjusted-R2, 8th is best
rs$adjr2
# BIC, 2nd is best
# Note that this is not the same as our calculated BIC
rs$bic
#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8
AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best
par(mfrow=c(2,2))
plot(msize, rs$adjr2, xlab="No. of Parameters", ylab = "Adjusted Rsquare");
plot(msize, rs$cp, xlab="No. of Parameters", ylab = "Mallow's Cp");
plot(msize, AIC, xlab="No. of Parameters", ylab = "AIC");
plot(msize, BIC, xlab="No. of Parameters", ylab = "BIC");
#Determining which variables to keep based
#Because both Adjusted R2 and AIC suggested 8 variables, we will choose 8 variables
rs$which[8,]
select.var = colnames(rs$which)[rs$which[8,]]
select.var = select.var[-1]
#fitting model
criteria_fit <- lm(log_physicians ~ . , data=train[, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_physicians", "log_percap_income", "log_hospital_beds")])
#Using RMSE function from earlier we will calculate errors
#Train RMSE
rmse(fitted(criteria_fit), train$log_physicians)
#Test RMSE
rmse(predict(criteria_fit, test), test$log_physicians)
results[2,1] <- "Citerion Selected Model"
results[2,2] <- rmse(fitted(criteria_fit), train$log_physicians)
results[2,3] <- rmse(predict(criteria_fit, test), test$log_physicians)
results[2, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_physicians", "log_percap_income", "log_hospital_beds")] <- TRUE
results[2,c(4,5,7,9)]
library(tidyverse)
library(readr)
library(leaps)
library(lars)
library(pls)
# Reading Transformed Data In From CS1
sub1_data<-read.csv("sub1_data")
# Convert Region to factor level
sub1_data <- sub1_data %>% mutate(region = as.factor(region))
# Setting Seed
set.seed(425)
# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(sub1_data), replace=TRUE, prob=c(0.7, 0.3))
train <- sub1_data[sample, ]
test <- sub1_data[!sample, ]
# Create Table for Results
results <- data.frame(matrix(ncol = 12, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
#Creating model using training data
cs1_model<-lm(log_physicians ~ ., data=train)
#Creating function to calculate RMSE
rmse<-function(x,y) sqrt(mean((x-y)^2))
#Train MSE
rmse(fitted(cs1_model), train$log_physicians)
#Test MSE
rmse(predict(cs1_model, test), test$log_physicians)
results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:12] <- TRUE
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)
# Adjusted-R2, 8th is best
rs$adjr2
# BIC, 2nd is best
# Note that this is not the same as our calculated BIC
rs$bic
#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8
AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best
par(mfrow=c(2,2))
plot(msize, rs$adjr2, xlab="No. of Parameters", ylab = "Adjusted Rsquare");
plot(msize, rs$cp, xlab="No. of Parameters", ylab = "Mallow's Cp");
plot(msize, AIC, xlab="No. of Parameters", ylab = "AIC");
plot(msize, BIC, xlab="No. of Parameters", ylab = "BIC");
#Determining which variables to keep based
#Because both Adjusted R2 and AIC suggested 8 variables, we will choose 8 variables
rs$which[8,]
select.var = colnames(rs$which)[rs$which[8,]]
select.var = select.var[-1]
#fitting model
criteria_fit <- lm(log_physicians ~ . , data=train[, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_physicians", "log_percap_income", "log_hospital_beds")])
#Using RMSE function from earlier we will calculate errors
#Train RMSE
rmse(fitted(criteria_fit), train$log_physicians)
#Test RMSE
rmse(predict(criteria_fit, test), test$log_physicians)
results[2,1] <- "Citerion Selected Model"
results[2,2] <- rmse(fitted(criteria_fit), train$log_physicians)
results[2,3] <- rmse(predict(criteria_fit, test), test$log_physicians)
results[2, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")] <- TRUE
results[2,c(4,7)]
results[2,c(4,7)] <- FALSE
#standardize df
phys_train <- train %>% mutate_all(~(scale(.) %>% as.vector))
library(tidyverse)
library(readr)
library(leaps)
library(lars)
library(pls)
# Reading Transformed Data In From CS1
sub1_data<-read.csv("sub1_data")
# Convert Region to factor level
sub1_data <- sub1_data %>% mutate(region = as.factor(region))
# Setting Seed
set.seed(425)
# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(sub1_data), replace=TRUE, prob=c(0.7, 0.3))
train <- sub1_data[sample, ]
test <- sub1_data[!sample, ]
# Create Table for Results
results <- data.frame(matrix(ncol = 12, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
#Creating model using training data
cs1_model<-lm(log_physicians ~ ., data=train)
#Creating function to calculate RMSE
rmse<-function(x,y) sqrt(mean((x-y)^2))
#Train MSE
rmse(fitted(cs1_model), train$log_physicians)
#Test MSE
rmse(predict(cs1_model, test), test$log_physicians)
results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:12] <- TRUE
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)
# Adjusted-R2, 8th is best
rs$adjr2
# BIC, 2nd is best
# Note that this is not the same as our calculated BIC
rs$bic
#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8
AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best
par(mfrow=c(2,2))
plot(msize, rs$adjr2, xlab="No. of Parameters", ylab = "Adjusted Rsquare");
plot(msize, rs$cp, xlab="No. of Parameters", ylab = "Mallow's Cp");
plot(msize, AIC, xlab="No. of Parameters", ylab = "AIC");
plot(msize, BIC, xlab="No. of Parameters", ylab = "BIC");
#Determining which variables to keep based
#Because both Adjusted R2 and AIC suggested 8 variables, we will choose 8 variables
rs$which[8,]
select.var = colnames(rs$which)[rs$which[8,]]
select.var = select.var[-1]
#fitting model
criteria_fit <- lm(log_physicians ~ . , data=train[, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_physicians", "log_percap_income", "log_hospital_beds")])
#Using RMSE function from earlier we will calculate errors
#Train RMSE
rmse(fitted(criteria_fit), train$log_physicians)
#Test RMSE
rmse(predict(criteria_fit, test), test$log_physicians)
results[2,1] <- "Citerion Selected Model"
results[2,2] <- rmse(fitted(criteria_fit), train$log_physicians)
results[2,3] <- rmse(predict(criteria_fit, test), test$log_physicians)
results[2, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")] <- TRUE
results[2,c(4,7)] <- FALSE
model_train <- train
model_test <- test
model_train$region2 <- ifelse(train$region == 2, 1, 0)
model_test$region2 <- ifelse(test$region == 2, 1, 0)
model_train$region3 <- ifelse(train$region == 3, 1, 0)
model_test$region3 <- ifelse(test$region == 3, 1, 0)
model_train$region4 <- ifelse(train$region == 4, 1, 0)
model_test$region4 <- ifelse(test$region == 4, 1, 0)
model_train <- data.frame(model_train %>%
dplyr::select(-region))
model_test <- data.frame(model_test %>%
dplyr::select(-region))
#standardize df
phys_train <- model_train %>% mutate_all(~(scale(.) %>% as.vector))
phys_test <- model_test %>% mutate_all(~(scale(.) %>% as.vector))
phys.ridge <- lm.ridge(log_physicians~., phys_train, lambda=seq(0, 100, len=100))
which.min(phys.ridge$GCV)
matplot(phys.ridge$lambda, coef(phys.ridge), type="l", xlab=expression(lambda), ylab=expression(hat(beta)), col=1)
abline(v=2.020202)
phys.pred.train <- cbind(1, as.matrix(phys_train[,-5]))%*% coef(phys.ridge)[8,]
rmse(phys.pred.train, phys_train$log_physicians)
phys.pred.test <- cbind(1, as.matrix(phys_test[,-5]))%*% coef(phys.ridge)[8,]
rmse(phys.pred.test, phys_test$log_physicians)
results[1,1] <- "Ridge Model"
results[1,2] <- rmse(phys.pred.train, phys_train$log_physicians)
results[1,3] <- rmse(phys.pred.test, phys_test$log_physicians)
results[1,4:12] <- TRUE
results
coef(lasso, s=svm, mode="fraction")
coef(physlasso, s=svm, mode="fraction")
results
library(tidyverse)
library(readr)
library(leaps)
library(lars)
library(pls)
library(MASS)
# Reading Transformed Data In From CS1
sub1_data<-read.csv("sub1_data")
# Convert Region to factor level
sub1_data <- sub1_data %>% mutate(region = as.factor(region))
# Setting Seed
set.seed(425)
# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(sub1_data), replace=TRUE, prob=c(0.7, 0.3))
train <- sub1_data[sample, ]
test <- sub1_data[!sample, ]
# Create Table for Results
results <- data.frame(matrix(ncol = 12, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
#Creating model using training data
cs1_model<-lm(log_physicians ~ ., data=train)
#Creating function to calculate RMSE
rmse<-function(x,y) sqrt(mean((x-y)^2))
#Train MSE
rmse(fitted(cs1_model), train$log_physicians)
#Test MSE
rmse(predict(cs1_model, test), test$log_physicians)
results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:12] <- TRUE
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)
# Adjusted-R2, 8th is best
rs$adjr2
# BIC, 2nd is best
# Note that this is not the same as our calculated BIC
rs$bic
#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8
AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best
par(mfrow=c(2,2))
plot(msize, rs$adjr2, xlab="No. of Parameters", ylab = "Adjusted Rsquare");
plot(msize, rs$cp, xlab="No. of Parameters", ylab = "Mallow's Cp");
plot(msize, AIC, xlab="No. of Parameters", ylab = "AIC");
plot(msize, BIC, xlab="No. of Parameters", ylab = "BIC");
#Determining which variables to keep based
#Because both Adjusted R2 and AIC suggested 8 variables, we will choose 8 variables
rs$which[8,]
select.var = colnames(rs$which)[rs$which[8,]]
select.var = select.var[-1]
#fitting model
criteria_fit <- lm(log_physicians ~ . , data=train[, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_physicians", "log_percap_income", "log_hospital_beds")])
#Using RMSE function from earlier we will calculate errors
#Train RMSE
rmse(fitted(criteria_fit), train$log_physicians)
#Test RMSE
rmse(predict(criteria_fit, test), test$log_physicians)
results[2,1] <- "Citerion Selected Model"
results[2,2] <- rmse(fitted(criteria_fit), train$log_physicians)
results[2,3] <- rmse(predict(criteria_fit, test), test$log_physicians)
results[2, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")] <- TRUE
results[2,c(4,7)] <- FALSE
# Load Libraries
library(tidyverse)
library(leaps)
library(pls)
library(MASS)
library(lars)
# Load Data
crime <- read_table("crime.txt")
# Check For NA's and INF's
complete_rows <- crime[complete.cases(crime), ]
nrow(crime) == nrow(complete_rows)
# Convert factor col
crime <- crime %>%
mutate(So = as.factor(So))
# Check Correlation
cor(crime[,-2])
# 3 highly correlated (Po1, Po2, and Wealth)
# 2 highly correlated (U1, U2)
# 2 highly correlated (Ed, Ineq)
# Keeping Po1, U2, and Ed because they are highest with target
crimeNoCor <- crime %>%
dplyr::select(-Po2, -Wealth, -U1, -Ineq)
#make this example reproducible
set.seed(425)
# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(crime), replace=TRUE, prob=c(0.7, 0.3))
train <- crime[sample, ]
test <- crime[!sample, ]
trainNoCor <- crimeNoCor[sample, ]
testNoCor <- crimeNoCor[!sample, ]
# Run leaps and bounds
subset_selection = regsubsets(Crime~., data = trainNoCor)
rs = summary(subset_selection)
rs$bic #Model 8
rs
model <- lm(Crime~So+NW+Pop+LF+U2+Po1+Ed+Prob, data = trainNoCor)
summary(model)
# Define RMSE function
rmse <- function(x,y) sqrt(mean((x-y)^2))
# Train RMSE
rmse(predict(model, trainNoCor), trainNoCor$Crime)
# Test RMSE
rmse(predict(model, testNoCor), testNoCor$Crime)
#make this example reproducible
set.seed(425)
train <- train %>%
mutate(So = as.numeric(So))
test <- test %>%
mutate(So = as.numeric(So))
pcrcv <- pcr(Crime~., data = train, scale = TRUE, validation = "LOO", ncomp = 15)
pcrCV <- RMSEP(pcrcv, estimate="CV")
plot(pcrCV)
which.min(pcrCV$val) #8 components chosen by lowest CV RMSE
# Training RMSE
rmse(predict(pcrcv, train, ncom = 8), train$Crime)
# Get test RMSE
rmse(predict(pcrcv, test, ncom = 8), test$Crime)
#make this example reproducible
set.seed(425)
train <- trainNoCor %>%
mutate(So = as.numeric(So))
test <- testNoCor %>%
mutate(So = as.numeric(So))
# Choosing LASSO because it performs model selection
train.y <- train$Crime
train.x <- as.matrix(train[,-12])
test.y <- test$Crime
test.x <- as.matrix(test[,-12])
lasso <- lars(train.x, train.y)
cv.ml <- cv.lars(train.x,train.y)
which.min(cv.ml$cv)
svm <- cv.ml$index[which.min(cv.ml$cv)]
svm
plot(lasso)
# Train MSE
predlasso <- predict(lasso, train.x, s = svm, mode = "fraction")
rmse(train$Crime, predlasso$fit)
# Test MSE
predlasso <- predict(lasso, test.x, s = svm, mode = "fraction")
rmse(test$Crime, predlasso$fit)
coef(lasso, s=svm, mode="fraction")
# Run leaps and bounds
subset_selection = regsubsets(Crime~., data = trainNoCor)
rs = summary(subset_selection)
rs$bic #Model 8
rs
model <- lm(Crime~So+NW+Pop+LF+U2+Po1+Ed+Prob, data = trainNoCor)
summary(model)
# Define RMSE function
rmse <- function(x,y) sqrt(mean((x-y)^2))
# Train RMSE
rmse(predict(model, trainNoCor), trainNoCor$Crime)
# Test RMSE
rmse(predict(model, testNoCor), testNoCor$Crime)
summary(lasso, s=svm, mode="fraction")
coef(lasso, s=svm, mode="fraction")
coef(lasso, s=svm, mode="fraction")
#make this example reproducible
set.seed(425)
train <- trainNoCor %>%
mutate(So = as.numeric(So))
test <- testNoCor %>%
mutate(So = as.numeric(So))
# Choosing LASSO because it performs model selection
train.y <- train$Crime
train.x <- as.matrix(train[,-12])
test.y <- test$Crime
test.x <- as.matrix(test[,-12])
lasso <- lars(train.x, train.y)
cv.ml <- cv.lars(train.x,train.y)
which.min(cv.ml$cv)
svm <- cv.ml$index[which.min(cv.ml$cv)]
svm
plot(lasso)
# Train MSE
predlasso <- predict(lasso, train.x, s = svm, mode = "fraction")
rmse(train$Crime, predlasso$fit)
# Test MSE
predlasso <- predict(lasso, test.x, s = svm, mode = "fraction")
rmse(test$Crime, predlasso$fit)
coef(lasso, s=svm, mode="fraction")
runApp('C:/Users/busti/OneDrive/Desktop/jbustin2_Wednesday/final_project')
