---
title: "Stat 425 Case Study 1"
author: "Carrie Mecca, Charlie Marcou, Jack Hanley, Jessie Bustin"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r Loading Data & Libraries}
# Libraries
library(tidyverse)
library(faraway)
library(lmtest)
library(MASS)

# Load Data & Name Columns
data <- read_table("CDI.txt", col_names = FALSE)
data <- data %>%
  rename(id = X1,
         county = X2,
         state = X3,
         land_area = X4,
         total_pop = X5,
         pop_18to24 = X6,
         pop_over65 = X7,
         num_physicians = X8,
         num_hospital_beds = X9,
         serious_crimes = X10,
         highschool_rate = X11,
         bachelors_rate = X12,
         poverty_rate = X13,
         unemployment_rate = X14,
         per_capita_income = X15,
         total_personal_income = X16,
         region = X17)
```

```{r EDA v1}
# Check Variable Types
str(data)

# Check for NA's and INF's
complete_rows <- data[complete.cases(data), ]
nrow(data) == nrow(complete_rows)

# Change region type because it is not numeric
data <- data %>%
  mutate(region = as.factor(region))

# Check correlation for numeric features
data %>%
  dplyr::select(-id, -county, -state, -region) %>%
  cor() %>%
  round(digits = 2)

# We have 4 variables highly correlated with total_pop so will transform them to per 100,000 people for 2 (one is our target so we will leave that alone)
# Will drop total income because we already have per capita
# We can also drop the ID column
model_data <- data.frame(data %>%
  dplyr::select(-id, -total_personal_income) %>%
  mutate(hospital_beds_percap = num_hospital_beds / (total_pop / 100000),
         serious_crimes_percap = serious_crimes / (total_pop / 100000)) %>%
  dplyr::select(-num_hospital_beds, -serious_crimes))

# Check county levels
model_data %>%
  group_by(county) %>%
  summarise(counts = n()) %>%
  summarise(min = min(counts), max(counts))

# Drop Counties and check states
model_data <- model_data %>%
  dplyr::select(-county)
model_data %>%
  group_by(state) %>%
  summarise(counts = n()) %>%
  arrange(counts) %>%
  head()

# Drop States and we will use regions
model_data <- model_data %>%
  dplyr::select(-state)

# Recheck correlation
model_data %>%
  dplyr::select(-region) %>%
  cor() %>%
  round(digits = 2)

## Let's also look at a scattermatrix for correlation
model_data %>% 
  dplyr::select(-region) %>%
  pairs()

# With a cutoff of 0.75 we have no highly correlated pairs other than population and our target

# With a cutoff of 0.7 we have the 2 following pairs:
# high_school_rate & bachelors_rate
# bachelors_rate & per_capita_income
```

```{r Full Model Build v1}
# Start by just looking at total_pop for fun
slr_model <- lm(num_physicians~total_pop, data = model_data)
summary(slr_model)

# Full Model
mlr_full_model <- lm(num_physicians~., data = model_data)
summary(mlr_full_model)
```

```{r Testing Based Model Selection v1}
# Remove some fields based on EDA and full model summary
sub1_data <- model_data %>%
  dplyr::select(-pop_over65, -unemployment_rate, -highschool_rate)
mlr_sub1_model <- lm(num_physicians~., data = sub1_data)
anova(mlr_sub1_model, mlr_full_model)
```

```{r Leverages v1}
## Checking high-leverage points
leverages=lm.influence(mlr_sub1_model)$hat
head(leverages)
## Plot to help identify high leverage observations
halfnorm(leverages, nlab=6, labs=as.character(1:length(leverages)), ylab="Leverages")
## Determining leverages that exceed a 2p/n threshold
n = dim(model_data)[1]
p = length(variable.names(mlr_sub1_model))
leverages.high = leverages[leverages>(2*p/n)]
leverages.high
## We currently have many high leverage points (29), They represent only about 6.6% of the data.
## Before continuing, let us look at what high leverage points are good and bad
## Calculate IQR for number of physicians 
IQR_y = IQR(model_data$num_physicians)
## Define range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(model_data$num_physicians,0.25)
QT3_y = quantile(model_data$num_physicians,0.75)
lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y
vector_lim_y = c(lower_lim_y,upper_lim_y)
## Range for number of physicians
vector_lim_y
## Extract observations with high leverage points from the original data frame 
highlev = model_data[leverages>2*p/n,]
## Select only the observations with leverage points outside the range 
highlev_lower = highlev[highlev$num_physicians < vector_lim_y[1], ]
highlev_upper = highlev[highlev$num_physicians > vector_lim_y[2], ]
highlev2 = rbind(highlev_lower,highlev_upper)
## This is not outputting the observation number like her example did. It is probably because we're using a tibble. If someone wants to troubleshoot this that would be great. I will take another look later - Charlie
##I switched model_data to be a dataframe which I believe solves this issue-Carrie
highlev2
```

```{r outliers v1}
## Computing Studentized Residuals 
mlr_sub1_model.resid = rstudent(mlr_sub1_model); 
## Critical value with Bonferroni correction 
## Note: Compare to t-value later at the alpha we choose
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv
## Sorting residuals to find outliers
mlr_sub1_model.resid.sorted = sort(abs(mlr_sub1_model.resid), decreasing=TRUE)[1:10]
print(mlr_sub1_model.resid.sorted)
## Printing those above the value
## We can see observations 50, 67, 48, 19, 8, 53, and 11 are outliers. 
mlr_sub1_model.outliers = mlr_sub1_model.resid.sorted[abs(mlr_sub1_model.resid.sorted) > abs(bonferroni_cv)]
print(mlr_sub1_model.outliers)
```


```{r influence v1}
## Finding high cook's distance observations
mlr_sub1_model.cooks = cooks.distance(mlr_sub1_model)
sort(mlr_sub1_model.cooks, decreasing = TRUE)[1:10]
## Plotting cook's distance
plot(mlr_sub1_model.cooks)
## Some observations have high cook's distance relative to other observations, but none have cook's d > 1
```


```{r model assumptions v1}
## Checking Constant Variance
plot(mlr_sub1_model, which=1)
bptest(mlr_sub1_model)
## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub1_model, which=2)
hist(mlr_sub1_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub1_model$residuals, 'pnorm')  ## We may want to check that this is the right syntax for this since she didn't have any examples using the ks test
## Next step is to check linearity of each variable
```


```{r variable linearity v1}
checkLinearity <- function(var) {
  var_idx = which( colnames(sub1_data)==var )
  y.var = update(mlr_sub1_model, .~. -c(var_idx))$res
  #remove response + the variable itself 
  x.var = lm(sub1_data[,var_idx] ~ . ,sub1_data[,-c(var_idx,4)])$res
  
  plot(x.var, y.var, xlab=paste(var," Residuals"), ylab="Num Physicians Residuals",   col='Darkblue', pch=3)
  abline(lm(y.var ~ x.var), col='Darkblue', lwd=2)
  abline(v = 0, col="red", lty=3)
  abline(h = 0, col="red", lty=3)
}
predictors = names(sub1_data)
#remove the response variable (and region since it's a factor (?))
predictors = predictors[!(predictors %in% c("num_physicians","region"))]
#check linearity for each predictor
for (var in predictors) {
  checkLinearity(var)
}
```

```{r Log Linear Transformation}
# log transform target - Didn't help
sub2_data <- sub1_data %>%
  mutate(log_physicians = log(num_physicians))
mlr_sub2_model <- lm(log_physicians~., data = sub2_data)

# Checking Box Cox
physician.transformation = boxcox(mlr_sub1_model, lambda=seq(-2,2, length=400))

lambda <- physician.transformation$x[which.max(physician.transformation$y)]
lambda

# Using 0.1 for box cox - Didn't help
lambda <- 0.1
sub2_data <- sub1_data %>%
  mutate(boxcox_physicians = (num_physicians^lambda - 1)/ lambda)
mlr_sub2_model <- lm(boxcox_physicians~., data = sub2_data)

## Checking Constant Variance
plot(mlr_sub2_model, which=1)
library(lmtest)
bptest(mlr_sub2_model)
## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub2_model, which=2)
hist(mlr_sub2_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub2_model$residuals, 'pnorm')
```

```{r Univariate EDA v2}
# Let's look at each variable and the target graphically
# Need a log transformation on land_area
model_data %>% ggplot(aes(x = log(land_area), y = log(num_physicians))) +
  geom_point() 

# log of total_pop will do wonders!!!
model_data %>% ggplot(aes(x = log(total_pop), y = log(num_physicians))) +
  geom_point() 

# looks ok as is 
model_data %>% ggplot(aes(x = pop_18to24, y = log(num_physicians))) +
  geom_point() 

# looks ok as is
model_data %>% ggplot(aes(x = pop_over65, y = log(num_physicians))) +
  geom_point() 

# looks ok as is - tried 1/x, x + x^2, sqrt and log but non look better
model_data %>% ggplot(aes(x = highschool_rate, y = log(num_physicians))) +
  geom_point() 

# looks ok as is but log looks better
model_data %>% ggplot(aes(x = log(bachelors_rate), y = log(num_physicians))) +
  geom_point() 

# looks ok as is
model_data %>% ggplot(aes(x = poverty_rate, y = log(num_physicians))) +
  geom_point() 

# looks ok as is
model_data %>% ggplot(aes(x = unemployment_rate, y = log(num_physicians))) +
  geom_point() 

# looks ok as is but log is better
model_data %>% ggplot(aes(x = log(per_capita_income), y = log(num_physicians))) +
  geom_point() 

# log helps!
model_data %>% ggplot(aes(x = log(hospital_beds_percap), y = log(num_physicians))) +
  geom_point() 

# looks ok as is but log is better
model_data %>% ggplot(aes(x = log(serious_crimes_percap), y = log(num_physicians))) +
  geom_point() 

# log transformations based on above EDA
transformed_data <- model_data %>%
  mutate(log_physicians = log(num_physicians),
         log_pop = log(total_pop),
         log_land = log(land_area),
         log_bachelors = log(bachelors_rate),
         log_percap_income = log(per_capita_income),
         log_hospital_beds = log(hospital_beds_percap),
         log_crimes = log(serious_crimes_percap)) %>%
  dplyr::select(-c(num_physicians, total_pop, land_area, bachelors_rate, per_capita_income,
            hospital_beds_percap, serious_crimes_percap))
```

```{r Full Model Build v2}
# Start by just looking at total_pop for fun
slr_model <- lm(log_physicians~log_pop, data = transformed_data)
summary(slr_model)
# Full Model
mlr_full_model <- lm(log_physicians~., data = transformed_data)
summary(mlr_full_model)
```

```{r Testing Based Model Selection v2}
# Remove some fields based on EDA and full model summary
# The final group here was chosen based on EDA, p-values, and trial and error
sub1_data <- transformed_data %>%
  dplyr::select(-highschool_rate, -log_crimes, -log_land)
mlr_sub1_model <- lm(log_physicians~., data = sub1_data)
summary(mlr_sub1_model)
anova(mlr_sub1_model, mlr_full_model)

# Adding a test for slr model vs full to show the the slr is rejected
anova(slr_model, mlr_full_model)
```

```{r Leverages v2}
## Checking high-leverage points
leverages=lm.influence(mlr_sub1_model)$hat
head(leverages)
## Plot to help identify high leverage observations
halfnorm(leverages, nlab=6, labs=as.character(1:length(leverages)), ylab="Leverages")
## Determining leverages that exceed a 2p/n threshold
n = dim(model_data)[1]
p = length(variable.names(mlr_sub1_model))
leverages.high = leverages[leverages>(2*p/n)]
leverages.high
## We currently have many high leverage points (30)
## Before continuing, let us look at what high leverage points are good and bad
## Calculate IQR for number of physicians 
IQR_y = IQR(model_data$num_physicians)
## Define range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(model_data$num_physicians,0.25)
QT3_y = quantile(model_data$num_physicians,0.75)
lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y
vector_lim_y = c(lower_lim_y,upper_lim_y)
## Range for number of physicians
vector_lim_y
## Extract observations with high leverage points from the original data frame 
highlev = data[leverages>2*p/n,]
## Select only the observations with leverage points outside the range 
highlev_lower = highlev[highlev$num_physicians < vector_lim_y[1], ]
highlev_upper = highlev[highlev$num_physicians > vector_lim_y[2], ]
highlev2 = rbind(highlev_lower,highlev_upper)

# Only 3 bad high leverage points
highlev2
```


```{r outliers v2}
## Computing Studentized Residuals 
mlr_sub1_model.resid = rstudent(mlr_sub1_model); 
## Critical value with Bonferroni correction 
## Note: Compare to t-value later at the alpha we choose
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv
## Sorting residuals to find outliers
mlr_sub1_model.resid.sorted = sort(abs(mlr_sub1_model.resid), decreasing=TRUE)[1:10]
print(mlr_sub1_model.resid.sorted)
## 2 points are outliers (418, 42)
mlr_sub1_model.outliers = mlr_sub1_model.resid.sorted[abs(mlr_sub1_model.resid.sorted) > abs(bonferroni_cv)]
print(mlr_sub1_model.outliers)
```


```{r influence v2}
## Finding high cook's distance observations
mlr_sub1_model.cooks = cooks.distance(mlr_sub1_model)
sort(mlr_sub1_model.cooks, decreasing = TRUE)[1:10]
## Plotting cook's distance
plot(mlr_sub1_model.cooks)
## Some observations have high cook's distance relative to other observations, but none have cook's d > 1
```


```{r model assumptions v2}
## Checking Constant Variance
plot(mlr_sub1_model, which=1)
bptest(mlr_sub1_model)

## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub1_model, which=2)
hist(mlr_sub1_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub1_model$residuals, 'pnorm')  ## We may want to check that this is the right syntax for this since she didn't have any examples using the ks test
## Next step is to check linearity of each variable
```


```{r variable linearity v2}
sub1_data <- data.frame(sub1_data)

checkLinearity <- function(var) {
  var_idx = which( colnames(sub1_data)==var )
  y.var = update(mlr_sub1_model, .~. -c(var_idx))$res
  #remove response + the variable itself 
  x.var = lm(sub1_data[,var_idx] ~ . ,sub1_data[,-c(6)])$res
  
  plot(x.var, y.var, xlab=paste(var," Residuals"), ylab="Num Physicians Residuals",   col='Darkblue', pch=3)
  abline(lm(y.var ~ x.var), col='Darkblue', lwd=2)
  abline(v = 0, col="red", lty=3)
  abline(h = 0, col="red", lty=3)
}
predictors = names(sub1_data)
#remove the response variable (and region since it's a factor (?))
predictors = predictors[!(predictors %in% c("log_physicians","region"))]
#check linearity for each predictor
for (var in predictors) {
  checkLinearity(var)
}

```

<<<<<<< HEAD
=======
```{r Log Linear Transformation}
# log transform target - Didn't help
sub2_data <- sub1_data %>%
  mutate(log_physicians = log(num_physicians))
mlr_sub2_model <- lm(log_physicians~., data = sub2_data)

# Checking Box Cox
physician.transformation = boxcox(mlr_sub1_model, lambda=seq(-2,2, length=400))

lambda <- physician.transformation$x[which.max(physician.transformation$y)]
lambda

# Using 0.1 for box cox - Didn't help
lambda <- 0.1
sub2_data <- sub1_data %>%
  mutate(boxcox_physicians = (num_physicians^lambda - 1)/ lambda)
mlr_sub2_model <- lm(boxcox_physicians~., data = sub2_data)

## Checking Constant Variance
plot(mlr_sub2_model, which=1)
bptest(mlr_sub2_model)
## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub2_model, which=2)
hist(mlr_sub2_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub2_model$residuals, 'pnorm')
```

```{r Log Log Transformation}

```
>>>>>>> 8743247fadc5e7862f134e5c3e882eae473f2c70

