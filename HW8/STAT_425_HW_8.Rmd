---
title: "HW 8"
author: "Charlie Marcou, Carrie Mecca, Jessie Bustin"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r libraries}
library(tidyverse)
library(readr)
library(leaps)
library(lars)
library(pls)
library(MASS)
```

## 1) Train Test Split
```{r 1}
# Reading Transformed Data In From CS1
sub1_data<-read.csv("sub1_data")

# Convert Region to factor level
sub1_data <- sub1_data %>% mutate(region = as.factor(region))

# Setting Seed
set.seed(425)

# Train Test Split
sample <- sample(c(TRUE, FALSE), nrow(sub1_data), replace=TRUE, prob=c(0.7, 0.3))
train <- sub1_data[sample, ]
test <- sub1_data[!sample, ]

# Create Table for Results
results <- data.frame(matrix(ncol = 12, nrow = 5))
colnames(results) <- c("model", "trainRMSE", "testRMSE", "pop_18to24", "pop_over65", "poverty_rate", "unemployment_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")
```

## 2) 
```{r 2}
#Creating model using training data
cs1_model<-lm(log_physicians ~ ., data=train)

#Creating function to calculate RMSE
rmse<-function(x,y) sqrt(mean((x-y)^2))

#Train MSE
rmse(fitted(cs1_model), train$log_physicians)

#Test MSE
rmse(predict(cs1_model, test), test$log_physicians)

results[1,1] <- "Full Model"
results[1,2] <- rmse(fitted(cs1_model), train$log_physicians)
results[1,3] <- rmse(predict(cs1_model, test), test$log_physicians)
results[1,4:12] <- TRUE
```


## 3)

```{r problem 3}
regsubsets_selection=regsubsets(log_physicians~., data = train)
rs = summary(regsubsets_selection)

# Adjusted-R2, 8th is best
rs$adjr2

# BIC, 2nd is best
# Note that this is not the same as our calculated BIC
rs$bic

#We will compute AIC and BIC by hand
n=dim(train)[1]
msize = 1:8

AIC = n*log(rs$rss/n) + 2*msize;
which.min(AIC) #8 is best
BIC = n*log(rs$rss/n) + msize*log(n);
which.min(BIC) #6 is best

par(mfrow=c(2,2))
plot(msize, rs$adjr2, xlab="No. of Parameters", ylab = "Adjusted Rsquare");
plot(msize, rs$cp, xlab="No. of Parameters", ylab = "Mallow's Cp");
plot(msize, AIC, xlab="No. of Parameters", ylab = "AIC");
plot(msize, BIC, xlab="No. of Parameters", ylab = "BIC");

#Determining which variables to keep based
#Because both Adjusted R2 and AIC suggested 8 variables, we will choose 8 variables
rs$which[8,]

select.var = colnames(rs$which)[rs$which[8,]]

select.var = select.var[-1]

#fitting model
criteria_fit <- lm(log_physicians ~ . , data=train[, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_physicians", "log_percap_income", "log_hospital_beds")])

#Using RMSE function from earlier we will calculate errors
#Train RMSE
rmse(fitted(criteria_fit), train$log_physicians)

#Test RMSE
rmse(predict(criteria_fit, test), test$log_physicians)

results[2,1] <- "Citerion Selected Model"
results[2,2] <- rmse(fitted(criteria_fit), train$log_physicians)
results[2,3] <- rmse(predict(criteria_fit, test), test$log_physicians)
results[2, c("pop_over65", "poverty_rate", "region", "log_pop", "log_bachelors", "log_percap_income", "log_hospital_beds")] <- TRUE
results[2,c(4,7)] <- FALSE
```
```{r}
model_train <- train
model_test <- test

model_train$region2 <- ifelse(train$region == 2, 1, 0)
model_test$region2 <- ifelse(test$region == 2, 1, 0)
model_train$region3 <- ifelse(train$region == 3, 1, 0)
model_test$region3 <- ifelse(test$region == 3, 1, 0)
model_train$region4 <- ifelse(train$region == 4, 1, 0)
model_test$region4 <- ifelse(test$region == 4, 1, 0)

model_train <- data.frame(model_train %>%
  dplyr::select(-region))
model_test <- data.frame(model_test %>%
  dplyr::select(-region))

```

##4) Ridge

```{r problem 4}
#standardize df
phys_train <- model_train %>% mutate_all(~(scale(.) %>% as.vector))
phys_test <- model_test %>% mutate_all(~(scale(.) %>% as.vector))

phys.ridge <- lm.ridge(log_physicians~., phys_train, lambda=seq(0, 100, len=100))
which.min(phys.ridge$GCV)
matplot(phys.ridge$lambda, coef(phys.ridge), type="l", xlab=expression(lambda), ylab=expression(hat(beta)), col=1)
abline(v=2.020202)



phys.pred.train <- cbind(1, as.matrix(phys_train[,-5]))%*% coef(phys.ridge)[8,]

rmse(phys.pred.train, phys_train$log_physicians)

phys.pred.test <- cbind(1, as.matrix(phys_test[,-5]))%*% coef(phys.ridge)[8,]

rmse(phys.pred.test, phys_test$log_physicians)

results[3,1] <- "Ridge Model"
results[3,2] <- rmse(phys.pred.train, phys_train$log_physicians)
results[3,3] <- rmse(phys.pred.test, phys_test$log_physicians)
results[3,4:12] <- TRUE
```


##5) LASSO

```{r problem 5}
train.y<-model_train$log_physicians
train.x<-as.matrix(model_train[,-5])

test.x<-as.matrix(model_test[,-5])

physlasso<-lars(train.x,train.y)
cv.ml<-cv.lars(train.x,train.y)
which.min(cv.ml$cv)
svm<-cv.ml$index[which.min(cv.ml$cv)]
svm

predlasso_train <- predict(physlasso, train.x, s = svm, mode = "fraction")
rmse(model_train$log_physicians, predlasso_train$fit)

predlasso_test<-predict(physlasso, test.x, s=svm, mode="fraction")
rmse(predlasso_test$fit, model_test$log_physicians)

coef(physlasso, s=svm, mode="fraction")

results[4,1] <- "LASSO Model"
results[4,2] <- rmse(model_train$log_physicians, predlasso_train$fit)
results[4,3] <- rmse(predlasso_test$fit, model_test$log_physicians)
results[4,4:12] <- TRUE
```


##6) PCR
```{r problem 6}
phys.pcr<-pcr(log_physicians ~ ., scale=TRUE, data=model_train,ncomp=11)
summary(phys.pcr) 

#Based on the summary 6 components seems reasonable as it brings us to over 85% of the variation explained and each additional component seems to add less to that percentage.

rmse(predict(phys.pcr, ncomp=6), model_train$log_physicians) 
rmse(predict(phys.pcr, model_test, ncomp=6), model_test$log_physicians)

results[5,1] <- "Principal Componant Regression"
results[5,2] <- rmse(predict(phys.pcr, ncomp=6), model_train$log_physicians)
results[5,3] <- rmse(predict(phys.pcr, model_test, ncomp=6), model_test$log_physicians)
results
```

Above is a table with the summary of our analysis.  Our original model performed relatively well.  We attribute this to the feature engineering and selected we completed prior to completing case study 1.  We either transformed or removed highly correlated variables.  The criteria selected model using leaps and bounds removed 2 variables and the RMSE increased slightly. Chosing between these 2 models would come down to weighing model complexity over performance.  For the penalized regression models, ridge performed exceptionally well.  Overall, we would select the Ridge model as our ideal model due to the low train and test RMSE and that there isn't not a large difference in train verses test RMSE.  We were not surprised that LASSO returned the full model after tuning lambda.  Finally, the PCR model was the worst of the group.  We feel that it might have performed better if we had not done the feature selection and engineering in our EDA for case study 1.  In conclusion, we were satisfied with our full model but prefer the Ridge model as the best model.