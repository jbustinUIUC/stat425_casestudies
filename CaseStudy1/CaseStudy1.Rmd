---
title: "Stat 425 Case Study 1"
author: "Carrie Mecca, Charlie Marcou, Jack, Jessie Bustin"
date: "2022-10-08"
output: pdf_document
---

```{r Loading Data & Libraries}
# Libraries
library(tidyverse)
library(faraway)
# Load Data & Name Columns
data <- read_table("CDI.txt", col_names = FALSE)
data <- data %>%
  rename(id = X1,
         county = X2,
         state = X3,
         land_area = X4,
         total_pop = X5,
         pop_18to24 = X6,
         pop_over65 = X7,
         num_physicians = X8,
         num_hospital_beds = X9,
         serious_crimes = X10,
         highschool_rate = X11,
         bachelors_rate = X12,
         poverty_rate = X13,
         unemployment_rate = X14,
         per_capita_income = X15,
         total_personal_income = X16,
         region = X17)
```


```{r EDA}
# Check Variable Types
str(data)
# Check for NA's and INF's
complete_rows <- data[complete.cases(data), ]
nrow(data) == nrow(complete_rows)
# Change region type because it is not numeric
data <- data %>%
  mutate(region = as.factor(region))
# Check correlation for numeric features
data %>%
  select(-id, -county, -state, -region) %>%
  cor()
# We have 4 variables highly correlated with total_pop so will transform them to per 100,000 people for 2 (one is our target so we will leave that alone)
# Will drop total income because we already have per capita
# We can also drop the ID column
model_data <- data.frame(data %>%
  select(-id, -total_personal_income) %>%
  mutate(hospital_beds_percap = num_hospital_beds / (total_pop / 100000),
         serious_crimes_percap = serious_crimes / (total_pop / 100000)) %>%
  select(-num_hospital_beds, -serious_crimes))
# Check county levels
model_data %>%
  group_by(county) %>%
  summarise(counts = n()) %>%
  summarise(min = min(counts), max(counts))
# Drop Counties and check states
model_data <- model_data %>%
  select(-county)
model_data %>%
  group_by(state) %>%
  summarise(counts = n()) %>%
  arrange(counts) %>%
  head()
# Drop States and we will use regions
model_data <- model_data %>%
  select(-state)
# Recheck correlation
model_data %>%
  select(-region) %>%
  cor()
## Let's also look at a scattermatrix for correlation
model_data %>% 
  select(-region) %>%
  pairs()
# We should reevaluate here, in lecture she said to worry about over .7 or .75 so a few of these pairs can be ignored
# Highly correlated pairs:
# pop_18to24 & pop_over65
# high_school_rate & bachelors_rate
# high_school_rate & poverty_rate
# bachelors_rate & per_capita_income
# per_capita_rate & poverty_rate
# physicians_percap & total_pop - yay! heavy hitter for the target
# Predicting that we should drop high_school because bachelors is more important
```

```{r Full Model Build}
# Start by just looking at total_pop for fun
slr_model <- lm(num_physicians~total_pop, data = model_data)
summary(slr_model)
# Full Model
mlr_full_model <- lm(num_physicians~., data = model_data)
summary(mlr_full_model)
```

```{r Testing Based Model Selection}
# Remove some fields based on EDA and full model summary
sub1_data <- model_data %>%
  select(-pop_over65, -unemployment_rate, -highschool_rate)
mlr_sub1_model <- lm(num_physicians~., data = sub1_data)
anova(mlr_sub1_model, mlr_full_model)
```

```{r Leverages}
## Checking high-leverage points
leverages=lm.influence(mlr_sub1_model)$hat
head(leverages)
## Plot to help identify high leverage observations
halfnorm(leverages, nlab=6, labs=as.character(1:length(leverages)), ylab="Leverages")
## Determining leverages that exceed a 2p/n threshold
n = dim(model_data)[1]
p = length(variable.names(mlr_sub1_model))
leverages.high = leverages[leverages>(2*p/n)]
leverages.high
## We currently have many high leverage points (29), They represent only about 6.6% of the data.
## Before continuing, let us look at what high leverage points are good and bad
## Calculate IQR for number of physicians 
IQR_y = IQR(model_data$num_physicians)
## Define range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(model_data$num_physicians,0.25)
QT3_y = quantile(model_data$num_physicians,0.75)
lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y
vector_lim_y = c(lower_lim_y,upper_lim_y)
## Range for number of physicians
vector_lim_y
## Extract observations with high leverage points from the original data frame 
highlev = model_data[leverages>2*p/n,]
## Select only the observations with leverage points outside the range 
highlev_lower = highlev[highlev$num_physicians < vector_lim_y[1], ]
highlev_upper = highlev[highlev$num_physicians > vector_lim_y[2], ]
highlev2 = rbind(highlev_lower,highlev_upper)
## This is not outputting the observation number like her example did. It is probably because we're using a tibble. If someone wants to troubleshoot this that would be great. I will take another look later - Charlie
##I switched model_data to be a dataframe which I believe solves this issue-Carrie
highlev2
```


```{r outliers}
## Computing Studentized Residuals 
mlr_sub1_model.resid = rstudent(mlr_sub1_model); 
## Critical value with Bonferroni correction 
## Note: Compare to t-value later at the alpha we choose
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv
## Sorting residuals to find outliers
mlr_sub1_model.resid.sorted = sort(abs(mlr_sub1_model.resid), decreasing=TRUE)[1:10]
print(mlr_sub1_model.resid.sorted)
## Printing those above the value
## We can see observations 50, 67, 48, 19, 8, 53, and 11 are outliers. 
mlr_sub1_model.outliers = mlr_sub1_model.resid.sorted[abs(mlr_sub1_model.resid.sorted) > abs(bonferroni_cv)]
print(mlr_sub1_model.outliers)
```


```{r influence}
## Finding high cook's distance observations
mlr_sub1_model.cooks = cooks.distance(mlr_sub1_model)
sort(mlr_sub1_model.cooks, decreasing = TRUE)[1:10]
## Plotting cook's distance
plot(mlr_sub1_model.cooks)
## Some observations have high cook's distance relative to other observations, but none have cook's d > 1
```


```{r model assumptions}
## Checking Constant Variance
plot(mlr_sub1_model, which=1)
library(lmtest)
bptest(mlr_sub1_model)
## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub1_model, which=2)
hist(mlr_sub1_model$residuals)
ks.test(mlr_sub1_model$residuals) ## We want the k-s test because we have more than 50 observations, but the same syntax as the shapiro-wilk test does not work here. I couldn't find any example syntax for this test within her notes/slides.
## Next step is to check linearity of each variable
```


```{r variable linearity}
checkLinearity <- function(var) {
  var_idx = which( colnames(sub1_data)==var )

  y.var = update(mlr_sub1_model, .~. -c(var_idx))$res
  #remove response + the variable itself 
  x.var = lm(sub1_data[,var_idx] ~ . ,sub1_data[,-c(var_idx,4)])$res
  
  plot(x.var, y.var, xlab=paste(var," Residuals"), ylab="Num Physicians Residuals",   col='Darkblue', pch=3)
  abline(lm(y.var ~ x.var), col='Darkblue', lwd=2)
  abline(v = 0, col="red", lty=3)
  abline(h = 0, col="red", lty=3)
}
predictors = names(sub1_data)
#remove the response variable (and region since it's a factor (?))
predictors = predictors[!(predictors %in% c("num_physicians","region"))]

#check linearity for each predictor
for (var in predictors) {
  checkLinearity(var)
}



```
