---
title: "Stat 425 Case Study 1"
author: "Carrie Mecca, Charlie Marcou, Jack, Jessie Bustin"
date: "2022-10-08"
output: pdf_document
---

```{r Loading Data & Libraries}
# Libraries
library(tidyverse)
library(faraway)

# Load Data & Name Columns
data <- read_table("CDI.txt", col_names = FALSE)

data <- data %>%
  rename(id = X1,
         county = X2,
         state = X3,
         land_area = X4,
         total_pop = X5,
         pop_18to24 = X6,
         pop_over65 = X7,
         num_physicians = X8,
         num_hospital_beds = X9,
         serious_crimes = X10,
         highschool_rate = X11,
         bachelors_rate = X12,
         poverty_rate = X13,
         unemployment_rate = X14,
         per_capita_income = X15,
         total_personal_income = X16,
         region = X17)
```


```{r EDA}
# Check Variable Types
str(data)

# Check for NA's and INF's
complete_rows <- data[complete.cases(data), ]
nrow(data) == nrow(complete_rows)

# Change region type because it is not numeric
data <- data %>%
  mutate(region = as.factor(region))

# Check correlation for numeric features
data %>%
  select(-id, -county, -state, -region) %>%
  cor()

# We have 4 variables highly correlated with total_pop so will transform them to per 100,000 people for 2 (one is our target so we will leave that alone)
# Will drop total income because we already have per capita
# We can also drop the ID column
model_data <- data %>%
  select(-id, -total_personal_income) %>%
  mutate(hospital_beds_percap = num_hospital_beds / (total_pop / 100000),
         serious_crimes_percap = serious_crimes / (total_pop / 100000)) %>%
  select(-num_hospital_beds, -serious_crimes)

# Check county levels
model_data %>%
  group_by(county) %>%
  summarise(counts = n()) %>%
  summarise(min = min(counts), max(counts))

# Drop Counties and check states
model_data <- model_data %>%
  select(-county)

model_data %>%
  group_by(state) %>%
  summarise(counts = n()) %>%
  arrange(counts) %>%
  head()

# Drop States and we will use regions
model_data <- model_data %>%
  select(-state)

# Recheck correlation
model_data %>%
  select(-region) %>%
  cor()

## Let's also look at a scattermatrix for correlation
model_data %>% 
  select(-region) %>%
  pairs()

# We should reevaluate here, in lecture she said to worry about over .7 or .75 so a few of these pairs can be ignored

# Highly correlated pairs:
# pop_18to24 & pop_over65
# high_school_rate & bachelors_rate
# high_school_rate & poverty_rate
# bachelors_rate & per_capita_income
# per_capita_rate & poverty_rate
# physicians_percap & total_pop - yay! heavy hitter for the target

# Predicting that we should drop high_school because bachelors is more important
```

```{r Full Model Build}
# Start by just looking at total_pop for fun
slr_model <- lm(num_physicians~total_pop, data = model_data)

summary(slr_model)

# Full Model
mlr_full_model <- lm(num_physicians~., data = model_data)

summary(mlr_full_model)
```

```{r Testing Based Model Selection}
# Remove some fields based on EDA and full model summary
sub1_data <- model_data %>%
  select(-pop_over65, -unemployment_rate, -highschool_rate)

mlr_sub1_model <- lm(num_physicians~., data = sub1_data)

anova(mlr_sub1_model, mlr_full_model)
```

```{r Leverages}
## Checking high-leverage points

leverages=lm.influence(mlr_sub1_model)$hat
head(leverages)

## Plot to help identify high leverage observations
halfnorm(leverages, nlab=6, labs=as.character(1:length(leverages)), ylab="Leverages")

## Determining leverages that exceed a 2p/n threshold
n = dim(model_data)[1]
p = length(variable.names(mlr_sub1_model))
leverages.high = leverages[leverages>(2*p/n)]
leverages.high

## We currently have many high leverage points (29), They represent only about 6.6% of the data.

## Before continuing, let us look at what high leverage points are good and bad

## Calculate IQR for number of physicians 
IQR_y = IQR(model_data$num_physicians)

## Define range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(model_data$num_physicians,0.25)
QT3_y = quantile(model_data$num_physicians,0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y,upper_lim_y)

## Range for number of physicians
vector_lim_y

## Extract observations with high leverage points from the original data frame 
highlev = model_data[leverages>2*p/n,]

## Select only the observations with leverage points outside the range 
highlev_lower = highlev[highlev$num_physicians < vector_lim_y[1], ]
highlev_upper = highlev[highlev$num_physicians > vector_lim_y[2], ]
highlev2 = rbind(highlev_lower,highlev_upper)

## This is not outputting the observation number like her example did. It is probably because we're using a tibble. If someone wants to troubleshoot this that would be great. I will take another look later - Charlie
highlev2
```


```{r outliers}
## Computing Studentized Residuals 
mlr_sub1_model.resid = rstudent(mlr_sub1_model); 

## Critical value with Bonferroni correction 
## Note: Compare to t-value later at the alpha we choose
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv

## Sorting residuals to find outliers
mlr_sub1_model.resid.sorted = sort(abs(mlr_sub1_model.resid), decreasing=TRUE)[1:10]
print(mlr_sub1_model.resid.sorted)

## Printing those above the value
## We can see observations 50, 67, 48, 19, 8, 53, and 11 are outliers. 
mlr_sub1_model.outliers = mlr_sub1_model.resid.sorted[abs(mlr_sub1_model.resid.sorted) > abs(bonferroni_cv)]
print(mlr_sub1_model.outliers)
```


```{r influence}
## Finding high cook's distance observations
mlr_sub1_model.cooks = cooks.distance(mlr_sub1_model)
sort(mlr_sub1_model.cooks, decreasing = TRUE)[1:10]

## Plotting cook's distance
plot(mlr_sub1_model.cooks)

## Some observations have high cook's distance relative to other observations, but none have cook's d > 1
```


```{r model assumptions}

## Checking Constant Variance
plot(mlr_sub1_model, which=1)

library(lmtest)
bptest(mlr_sub1_model)

## Constant Variance seems to be violated

## Checking Normality
plot(mlr_sub1_model, which=2)
hist(mlr_sub1_model$residuals)

ks.test(mlr_sub1_model$residuals) ## We want the k-s test because we have more than 50 observations, but the same syntax as the shapiro-wilk test does not work here. I couldn't find any example syntax for this test within her notes/slides.


## Next step is to check linearity of each variable
```