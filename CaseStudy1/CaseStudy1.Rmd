---
title: "Stat 425 Case Study 1"
author: "Carrie Mecca, Charlie Marcou, Jack Hanley, Jessie Bustin"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r Loading Data & Libraries}
# Libraries
library(tidyverse)
library(faraway)
library(lmtest)
library(MASS)

# Load Data & Name Columns
data <- read_table("CDI.txt", col_names = FALSE)
data <- data %>%
  rename(id = X1,
         county = X2,
         state = X3,
         land_area = X4,
         total_pop = X5,
         pop_18to24 = X6,
         pop_over65 = X7,
         num_physicians = X8,
         num_hospital_beds = X9,
         serious_crimes = X10,
         highschool_rate = X11,
         bachelors_rate = X12,
         poverty_rate = X13,
         unemployment_rate = X14,
         per_capita_income = X15,
         total_personal_income = X16,
         region = X17)
```

```{r EDA v1}
# Check Variable Types
str(data)

# Check for NA's and INF's
complete_rows <- data[complete.cases(data), ]
nrow(data) == nrow(complete_rows)

# Change region type because it is not numeric
data <- data %>%
  mutate(region = as.factor(region))

# Check correlation for numeric features
data %>%
  dplyr::select(-id, -county, -state, -region) %>%
  cor() %>%
  round(digits = 2)

# We have 4 variables highly correlated with total_pop so will transform them to per 100,000 people for 2 (one is our target so we will leave that alone)
# Will drop total income because we already have per capita
# We can also drop the ID column
model_data <- data.frame(data %>%
  dplyr::select(-id, -total_personal_income) %>%
  mutate(hospital_beds_percap = num_hospital_beds / (total_pop / 100000),
         serious_crimes_percap = serious_crimes / (total_pop / 100000)) %>%
  dplyr::select(-num_hospital_beds, -serious_crimes))

# Check county levels
model_data %>%
  group_by(county) %>%
  summarise(counts = n()) %>%
  summarise(min = min(counts), max(counts))

# Drop Counties and check states
model_data <- model_data %>%
  dplyr::select(-county)
model_data %>%
  group_by(state) %>%
  summarise(counts = n()) %>%
  arrange(counts) %>%
  head()

# Drop States and we will use regions
model_data <- model_data %>%
  dplyr::select(-state)

# Recheck correlation
model_data %>%
  dplyr::select(-region) %>%
  cor() %>%
  round(digits = 2)

## Let's also look at a scattermatrix for correlation
model_data %>% 
  dplyr::select(-region) %>%
  pairs()

# With a cutoff of 0.75 we have no highly correlated pairs other than population and our target

# With a cutoff of 0.7 we have the 2 following pairs:
# high_school_rate & bachelors_rate
# bachelors_rate & per_capita_income
```

```{r Full Model Build v1}
# Start by just looking at total_pop for fun
slr_model <- lm(num_physicians~total_pop, data = model_data)
summary(slr_model)

# Full Model
mlr_full_model <- lm(num_physicians~., data = model_data)
summary(mlr_full_model)
```

```{r Testing Based Model Selection v1}
# Remove some fields based on EDA and full model summary
sub1_data <- model_data %>%
  dplyr::select(-pop_over65, -unemployment_rate, -highschool_rate)
mlr_sub1_model <- lm(num_physicians~., data = sub1_data)
anova(mlr_sub1_model, mlr_full_model)
```

```{r Leverages v1}
## Checking high-leverage points
leverages=lm.influence(mlr_sub1_model)$hat
head(leverages)
## Plot to help identify high leverage observations
halfnorm(leverages, nlab=6, labs=as.character(1:length(leverages)), ylab="Leverages")
## Determining leverages that exceed a 2p/n threshold
n = dim(model_data)[1]
p = length(variable.names(mlr_sub1_model))
leverages.high = leverages[leverages>(2*p/n)]
leverages.high
## We currently have many high leverage points (29), They represent only about 6.6% of the data.
## Before continuing, let us look at what high leverage points are good and bad
## Calculate IQR for number of physicians 
IQR_y = IQR(model_data$num_physicians)
## Define range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(model_data$num_physicians,0.25)
QT3_y = quantile(model_data$num_physicians,0.75)
lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y
vector_lim_y = c(lower_lim_y,upper_lim_y)
## Range for number of physicians
vector_lim_y
## Extract observations with high leverage points from the original data frame 
highlev = model_data[leverages>2*p/n,]
## Select only the observations with leverage points outside the range 
highlev_lower = highlev[highlev$num_physicians < vector_lim_y[1], ]
highlev_upper = highlev[highlev$num_physicians > vector_lim_y[2], ]
highlev2 = rbind(highlev_lower,highlev_upper)
## This is not outputting the observation number like her example did. It is probably because we're using a tibble. If someone wants to troubleshoot this that would be great. I will take another look later - Charlie
##I switched model_data to be a dataframe which I believe solves this issue-Carrie
highlev2
```

```{r outliers v1}
## Computing Studentized Residuals 
mlr_sub1_model.resid = rstudent(mlr_sub1_model); 
## Critical value with Bonferroni correction 
## Note: Compare to t-value later at the alpha we choose
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv
## Sorting residuals to find outliers
mlr_sub1_model.resid.sorted = sort(abs(mlr_sub1_model.resid), decreasing=TRUE)[1:10]
print(mlr_sub1_model.resid.sorted)
## Printing those above the value
## We can see observations 50, 67, 48, 19, 8, 53, and 11 are outliers. 
mlr_sub1_model.outliers = mlr_sub1_model.resid.sorted[abs(mlr_sub1_model.resid.sorted) > abs(bonferroni_cv)]
print(mlr_sub1_model.outliers)
```


```{r influence v1}
## Finding high cook's distance observations
mlr_sub1_model.cooks = cooks.distance(mlr_sub1_model)
sort(mlr_sub1_model.cooks, decreasing = TRUE)[1:10]
## Plotting cook's distance
plot(mlr_sub1_model.cooks)
## Some observations have high cook's distance relative to other observations, but none have cook's d > 1
```


```{r model assumptions v1}
## Checking Constant Variance
plot(mlr_sub1_model, which=1)
bptest(mlr_sub1_model)
## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub1_model, which=2)
hist(mlr_sub1_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub1_model$residuals, 'pnorm')  ## We may want to check that this is the right syntax for this since she didn't have any examples using the ks test
## Next step is to check linearity of each variable
```


```{r variable linearity v1}
checkLinearity <- function(var) {
  var_idx = which( colnames(sub1_data)==var )
  y.var = update(mlr_sub1_model, .~. -c(var_idx))$res
  #remove response + the variable itself 
  x.var = lm(sub1_data[,var_idx] ~ . ,sub1_data[,-c(var_idx,4)])$res
  
  plot(x.var, y.var, xlab=paste(var," Residuals"), ylab="Num Physicians Residuals",   col='Darkblue', pch=3)
  abline(lm(y.var ~ x.var), col='Darkblue', lwd=2,xlim = c(quantile(x.var,.005),quantile(x.var,.995)))
  abline(v = 0, col="red", lty=3)
  abline(h = 0, col="red", lty=3)
}
predictors = names(sub1_data)
#remove the response variable (and region since it's a factor (?))
predictors = predictors[!(predictors %in% c("num_physicians","region"))]
#check linearity for each predictor
for (var in predictors) {
  checkLinearity(var)
}
```

```{r Log Linear Transformation}
# log transform target - Didn't help
sub2_data <- sub1_data %>%
  mutate(log_physicians = log(num_physicians))
mlr_sub2_model <- lm(log_physicians~., data = sub2_data)

# Checking Box Cox
physician.transformation = boxcox(mlr_sub1_model, lambda=seq(-2,2, length=400))

lambda <- physician.transformation$x[which.max(physician.transformation$y)]
lambda

# Using 0.1 for box cox - Didn't help
lambda <- 0.1
sub2_data <- sub1_data %>%
  mutate(boxcox_physicians = (num_physicians^lambda - 1)/ lambda)
mlr_sub2_model <- lm(boxcox_physicians~., data = sub2_data)

## Checking Constant Variance
plot(mlr_sub2_model, which=1)
library(lmtest)
bptest(mlr_sub2_model)
## Constant Variance seems to be violated
## Checking Normality
plot(mlr_sub2_model, which=2)
hist(mlr_sub2_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub2_model$residuals, 'pnorm')
```

```{r Univariate EDA v2}
# Let's look at each variable and the target graphically
# Need a log transformation on land_area
model_data %>% ggplot(aes(x = log(land_area), y = log(num_physicians))) +
  geom_point() 

# log of total_pop will do wonders!!!
model_data %>% ggplot(aes(x = log(total_pop), y = log(num_physicians))) +
  geom_point() 

# looks ok as is 
model_data %>% ggplot(aes(x = pop_18to24, y = log(num_physicians))) +
  geom_point() 

# looks ok as is
model_data %>% ggplot(aes(x = pop_over65, y = log(num_physicians))) +
  geom_point() 

# looks ok as is - tried 1/x, x + x^2, sqrt and log but non look better
model_data %>% ggplot(aes(x = highschool_rate, y = log(num_physicians))) +
  geom_point() 

# looks ok as is but log looks better
model_data %>% ggplot(aes(x = log(bachelors_rate), y = log(num_physicians))) +
  geom_point() 

# looks ok as is
model_data %>% ggplot(aes(x = poverty_rate, y = log(num_physicians))) +
  geom_point() 

# looks ok as is
model_data %>% ggplot(aes(x = unemployment_rate, y = log(num_physicians))) +
  geom_point() 

# looks ok as is but log is better
model_data %>% ggplot(aes(x = log(per_capita_income), y = log(num_physicians))) +
  geom_point() 

# log helps!
model_data %>% ggplot(aes(x = log(hospital_beds_percap), y = log(num_physicians))) +
  geom_point() 

# looks ok as is but log is better
model_data %>% ggplot(aes(x = log(serious_crimes_percap), y = log(num_physicians))) +
  geom_point() 

# log transformations based on above EDA
transformed_data <- model_data %>%
  mutate(log_physicians = log(num_physicians),
         log_pop = log(total_pop),
         log_land = log(land_area),
         log_bachelors = log(bachelors_rate),
         log_percap_income = log(per_capita_income),
         log_hospital_beds = log(hospital_beds_percap),
         log_crimes = log(serious_crimes_percap)) %>%
  dplyr::select(-c(num_physicians, total_pop, land_area, bachelors_rate, per_capita_income,
            hospital_beds_percap, serious_crimes_percap))
```

```{r Full Model Build v2}
# Start by just looking at total_pop for fun
slr_model <- lm(log_physicians~log_pop, data = transformed_data)
summary(slr_model)
# Full Model
mlr_full_model <- lm(log_physicians~., data = transformed_data)
summary(mlr_full_model)
```

```{r Testing Based Model Selection v2}
# Remove some fields based on EDA and full model summary
# The final group here was chosen based on EDA, p-values, and trial and error
sub1_data <- transformed_data %>%
  dplyr::select(-highschool_rate, -log_crimes, -log_land)
mlr_sub1_model <- lm(log_physicians~., data = sub1_data)
summary(mlr_sub1_model)
anova(mlr_sub1_model, mlr_full_model)

# Adding a test for slr model vs full to show the the slr is rejected
anova(slr_model, mlr_full_model)
```

```{r Leverages v2}
## Checking high-leverage points
leverages=lm.influence(mlr_sub1_model)$hat
head(leverages)
## Plot to help identify high leverage observations
halfnorm(leverages, nlab=6, labs=as.character(1:length(leverages)), ylab="Leverages")
## Determining leverages that exceed a 2p/n threshold
n = dim(model_data)[1]
p = length(variable.names(mlr_sub1_model))
leverages.high = leverages[leverages>(2*p/n)]
leverages.high
## We currently have many high leverage points (30)
## Before continuing, let us look at what high leverage points are good and bad
## Calculate IQR for number of physicians 
IQR_y = IQR(model_data$num_physicians)
## Define range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(model_data$num_physicians,0.25)
QT3_y = quantile(model_data$num_physicians,0.75)
lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y
vector_lim_y = c(lower_lim_y,upper_lim_y)
## Range for number of physicians
vector_lim_y
## Extract observations with high leverage points from the original data frame 
highlev = data[leverages>2*p/n,]
## Select only the observations with leverage points outside the range 
highlev_lower = highlev[highlev$num_physicians < vector_lim_y[1], ]
highlev_upper = highlev[highlev$num_physicians > vector_lim_y[2], ]
highlev2 = rbind(highlev_lower,highlev_upper)

# Only 3 bad high leverage points
highlev2
```


```{r outliers v2}
## Computing Studentized Residuals 
mlr_sub1_model.resid = rstudent(mlr_sub1_model); 

## Critical value with Bonferroni correction 
## Note: Compare to t-value later at the alpha we choose
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv

## Sorting residuals to find outliers
mlr_sub1_model.resid.sorted = sort(abs(mlr_sub1_model.resid), decreasing=TRUE)[1:10]
print(mlr_sub1_model.resid.sorted)
## 2 points are outliers (418, 42)
mlr_sub1_model.outliers = mlr_sub1_model.resid.sorted[abs(mlr_sub1_model.resid.sorted) > abs(bonferroni_cv)]
print(mlr_sub1_model.outliers)
```


```{r influence v2}
## Finding high cook's distance observations
mlr_sub1_model.cooks = cooks.distance(mlr_sub1_model)
sort(mlr_sub1_model.cooks, decreasing = TRUE)[1:10]
## Plotting cook's distance
plot(mlr_sub1_model.cooks)
## Some observations have high cook's distance relative to other observations, but none have cook's d > 1
```


```{r model assumptions v2}
## Checking Constant Variance
plot(mlr_sub1_model, which=1)
bptest(mlr_sub1_model)

## Constant Variance seems to be violated based on p-value, the plot looks OK though
## Checking Normality
plot(mlr_sub1_model, which=2)
hist(mlr_sub1_model$residuals)
### We can use the KS test to assess normality because n>50. 
ks.test(mlr_sub1_model$residuals, 'pnorm')  ## We may want to check that this is the right syntax for this since she didn't have any examples using the ks test
## Next step is to check linearity of each variable
```


```{r variable linearity v2}
sub1_data <- data.frame(sub1_data)

checkLinearity <- function(var) {
  var_idx = which( colnames(sub1_data)==var )
  y.var = update(mlr_sub1_model, .~. -c(var_idx))$res
  #remove response + the variable itself 
  x.var = lm(sub1_data[,var_idx] ~ . ,sub1_data[,-c(6)])$res
  
  plot(x.var, y.var, xlab=paste(var," Residuals"), ylab="Num Physicians Residuals",   col='Darkblue', pch=3,xlim = c(quantile(x.var,.005),quantile(x.var,.995)))
  abline(lm(y.var ~ x.var), col='Darkblue', lwd=2)
  abline(v = 0, col="red", lty=3)
  abline(h = 0, col="red", lty=3)
}
predictors = names(sub1_data)
#remove the response variable (and region since it's a factor (?))
predictors = predictors[!(predictors %in% c("log_physicians","region"))]
#check linearity for each predictor
for (var in predictors) {
  checkLinearity(var)
}

```

```{r checking collinearity}

## Checking VIF
round(vif(mlr_sub1_model),3) #log_bachelors and log_percap_income are kind of concerning to me, but are not greater than her rule of thumb which is 10. If we really want we can test them out, but the condition number looks good. - Charlie

## Grabbing design matrix
x = model.matrix(mlr_sub1_model)[,-1]

## Standardize the matrix
x = x - matrix(apply(x,2, mean), 440,11, byrow=TRUE)
x = x / matrix(apply(x, 2, sd), 440,11, byrow=TRUE)


## Compute the eigenvalues of the matrix
eigenvalues.x = eigen(t(x) %*% x) 
eigenvalues.x$val

## Compute Condition Number
sqrt(eigenvalues.x$val[1]/eigenvalues.x$val[8]) ## Is less than 30, looks good.
```


